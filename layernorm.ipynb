{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
    "\n",
    "from torch.jit import Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormCustom(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6, elementwise_affine=False, bias=True, device=None, dtype=None):\n",
    "        super(LayerNormCustom, self).__init__()\n",
    "        if isinstance(normalized_shape, (tuple, list)):\n",
    "            self.normalized_shape = normalized_shape\n",
    "        else:\n",
    "            self.normalized_shape = (normalized_shape,)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # mean = x.mean(dim=-1, keepdim=True)\n",
    "        mean = torch.mean(x, dim=-1,keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n",
    "        # var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Basic Equivalence Test...\n",
      "Basic Equivalence Test passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Basic Equivalence Test...\")\n",
    "x = torch.randn(2, 3, 4, requires_grad=True)\n",
    "norm1 = nn.LayerNorm(4)\n",
    "norm2 = LayerNormCustom(4)\n",
    "output1 = norm1(x)\n",
    "output2 = norm2(x)\n",
    "assert torch.allclose(output1, output2, atol=1e-5), f\"Outputs are not close! {output1} {output2}\"\n",
    "print(\"Basic Equivalence Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Shape Variability Test...\n",
      "Shape Variability Test passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Shape Variability Test...\")\n",
    "shapes = [(4,), (2, 4), (2, 3, 4), (2, 3, 4, 5)]\n",
    "for shape in shapes:\n",
    "    x = torch.randn(*shape, requires_grad=True)\n",
    "    norm1 = nn.LayerNorm(shape[-1])\n",
    "    norm2 = LayerNormCustom(shape[-1])\n",
    "    assert torch.allclose(norm1(x), norm2(x), atol=1e-4), f\"Mismatch for shape {shape} nums {norm1(x)} {norm2(x)}\"\n",
    "print(\"Shape Variability Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradient Consistency Test...\n",
      "Gradient Consistency Test passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Gradient Consistency Test...\")\n",
    "x = torch.randn(2, 3, 4, requires_grad=True)\n",
    "norm1 = nn.LayerNorm(4)\n",
    "norm2 = LayerNormCustom(4)\n",
    "output1 = norm1(x)\n",
    "output2 = norm2(x)\n",
    "loss1 = output1.sum()\n",
    "loss2 = output2.sum()\n",
    "grad1 = torch.autograd.grad(loss1, x, create_graph=True)[0]\n",
    "grad2 = torch.autograd.grad(loss2, x, create_graph=True)[0]\n",
    "assert torch.allclose(grad1, grad2, atol=1e-6), \"Gradient mismatch!\"\n",
    "print(\"Gradient Consistency Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Edge Cases Test...\n",
      "Edge Cases Test passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Edge Cases Test...\")\n",
    "# All zeros\n",
    "x = torch.zeros(2, 3, 4)\n",
    "norm1 = nn.LayerNorm(4)\n",
    "norm2 = LayerNormCustom(4)\n",
    "assert torch.allclose(norm1(x), norm2(x), atol=1e-6), \"Mismatch for all zeros\"\n",
    "# Very large values\n",
    "x = torch.full((2, 3, 4), 1e6)\n",
    "assert torch.allclose(norm1(x), norm2(x), atol=1e-6), \"Mismatch for large values\"\n",
    "# Very small values\n",
    "x = torch.full((2, 3, 4), 1e-6)\n",
    "assert torch.allclose(norm1(x), norm2(x), atol=1e-6), \"Mismatch for small values\"\n",
    "print(\"Edge Cases Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Performance Test...\n",
      "nn.LayerNorm: 0.043641 seconds\n",
      "CustomLayerNorm: 0.247132 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Performance Test...\")\n",
    "x = torch.randn(512, 512, 512)\n",
    "norm1 = nn.LayerNorm(512)\n",
    "norm2 = LayerNormCustom(512)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "norm1(x)\n",
    "torch.cuda.synchronize()\n",
    "print(f\"nn.LayerNorm: {time.time() - start:.6f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "norm2(x)\n",
    "torch.cuda.synchronize()\n",
    "print(f\"CustomLayerNorm: {time.time() - start:.6f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
